import csv
from collections import defaultdict
from pathlib import Path

import numpy as np
import pytest
from lir.data.models import FeatureData, LLRData

from lrmodule import calculate_llrs
from lrmodule.data_types import ModelSettings, MarkType, ScoreType


def _parse_csv_input_file(input_file_path: Path) -> dict[str, list]:
    """Parse input data into dictionary which keys represent data type.

    Typically, a dictionary with the following keys is generated, due to
    the nature of the data, containing evaluation scores, known match scores
    and known non match scores.
    {
        "eval": [],
        "km": [],
        "knm": [],
    }
    """
    groups = defaultdict(list)

    with open(input_file_path) as f:
        reader = csv.reader(f)

        for type_, value in reader:
            groups[type_].append(float(value))

    return groups


def dump_llrs(path: Path, llrs: LLRData) -> None:
    with open(path, "w") as f:
        writer = csv.writer(f)
        for row in llrs.features:
            writer.writerow(row)


def parse_expected_output(expected_output_file_path: Path) -> LLRData:
    """Extract expected LLR data from the output file generated by Matlab code.

    The output file holds 5%-percentile, best estimate/median and 95%-percentile
    LLR values.

    The function provides a list of tuples, each tuple represents the 5%-percentile,
    best estimate/median and 95%-percentile LLR value.
    """
    # Keep a collection of LLR_data tuples in a list, which is returned at the end
    expected_llrs: list[list[float]] = []

    with open(expected_output_file_path) as f:
        reader = csv.reader(f)

        for row in reader:
            expected_llrs.append([float(value) for value in row])

    return LLRData(features=np.array(expected_llrs))


@pytest.mark.parametrize(
    "csv_file_name",
    [
        "aperture_shear-ccf-subset.csv",  # TODO: use sensible input for all model settings
    ],
)
def test_characterization_test_input_output_files(csv_file_name: str):
    """Check that for a given input, the expected output is provided."""
    # Given the input scores from a specific CSV
    input_data = _parse_csv_input_file(Path(__file__).parent / "score_input" / csv_file_name)
    evaluation_scores = np.array(input_data["eval"])
    known_match_scores = np.array(input_data["km"])
    known_non_match_scores = np.array(input_data["knm"])

    training_scores = np.concatenate([known_match_scores, known_non_match_scores])
    training_labels = np.concatenate([np.ones_like(known_match_scores), np.zeros_like(known_non_match_scores)])
    training_data = FeatureData(features=training_scores, labels=training_labels)

    # When we use this data to obtain LLRS
    model_settings = ModelSettings(MarkType.FIRING_PIN_IMPRESSION, ScoreType.ACCF)
    calculated_llrs = calculate_llrs(evaluation_scores, model_settings, training_data, None)

    # If the expected output is unknown, reset this test by creating the expected output file
    expected_llrs_file = Path(__file__).parent / "expected_llr_output" / csv_file_name
    if not expected_llrs_file.exists():
        dump_llrs(expected_llrs_file, calculated_llrs)
        pytest.skip(f"the expected LLR's were calculated and written to '{csv_file_name}'")

    # These should match the expected LLR values
    expected_llrs = parse_expected_output(expected_llrs_file)

    np.testing.assert_allclose(expected_llrs.features, calculated_llrs.features)
